{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nltk_download:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def download(self):\n",
    "        nltk.download(\"stopwords\")\n",
    "        self.stop_words=set(stopwords.words(\"english\"))\n",
    "        nltk.download('words')\n",
    "        self.words = set(nltk.corpus.words.words())\n",
    "        return self.stop_words,self.words\n",
    "class twitter_sentiment:\n",
    "    def __init__(self):\n",
    "        self.train=None\n",
    "        self.test=None\n",
    "        nltk_obj=nltk_download()\n",
    "        self.stop_words,self.words=nltk_obj.download()\n",
    "    def preprocess(self,train,length):\n",
    "        self.train=train\n",
    "        #self.test=test\n",
    "        self.remove_twitter_handles()\n",
    "        self.remove_punctuation()\n",
    "        self.remove_short_words(length)\n",
    "        self.tokenize()\n",
    "    def remove_pattern(self,input_txt, pattern):\n",
    "        r = re.findall(pattern, input_txt)\n",
    "        for i in r:\n",
    "            input_txt = re.sub(i, '', input_txt)\n",
    "        return input_txt\n",
    "    def remove_twitter_handles(self):\n",
    "        #self.test['processed_sentiment'] = np.vectorize(self.remove_pattern)(self.test['tweet'], \"@[\\w]*\")\n",
    "        self.train['processed_sentiment'] = np.vectorize(self.remove_pattern)(self.train['tweet'], \"@[\\w]*\")\n",
    "    \n",
    "    def remove_punctuation(self):\n",
    "        #self.test['processed_sentiment'] = self.test['processed_sentiment'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "        self.train['processed_sentiment'] = self.train['processed_sentiment'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    def  remove_short_words(self,length):\n",
    "        #self.test['processed_sentiment']=self.test['processed_sentiment'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>length]))\n",
    "        self.train['processed_sentiment']=self.train['processed_sentiment'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>length]))\n",
    "\n",
    "    def preprocess_test(self,data,length):\n",
    "        stemmer = PorterStemmer()\n",
    "        data['processed_sentiment'] = np.vectorize(self.remove_pattern)(data['tweet'], \"@[\\w]*\")\n",
    "        data['processed_sentiment'] = data['processed_sentiment'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "        data['processed_sentiment']=data['processed_sentiment'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>length]))\n",
    "        data_tokens = data['processed_sentiment'].apply(lambda x: x.split())\n",
    "        data_tokens=data_tokens.apply(lambda row:self.remove_stopwords(row))\n",
    "        data_tokens = data_tokens.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "        for i in range(len(data_tokens)):\n",
    "            data_tokens[i] = ' '.join(data_tokens[i])\n",
    "        data['processed_sentiment1'] = data_tokens\n",
    "        bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "        test_bow = bow_vectorizer.fit_transform(data['processed_sentiment1'])\n",
    "        return test_bow\n",
    "    def remove_stopwords(self,row):\n",
    "        result=[]\n",
    "        for elem in row:\n",
    "            if elem in self.stop_words:\n",
    "                pass\n",
    "            else:\n",
    "                result.append(elem)\n",
    "        return result\n",
    "    def remove_nonenglish(self,row):\n",
    "        result = []\n",
    "        for r in row: \n",
    "            if r in self.words: \n",
    "                result.append(r)\n",
    "        return result\n",
    "    def tokenize(self):\n",
    "        stemmer = PorterStemmer()\n",
    "        #test_tokens = self.test['processed_sentiment'].apply(lambda x: x.split())\n",
    "        train_tokens=self.train['processed_sentiment'].apply(lambda x: x.split())\n",
    "        train_tokens=train_tokens.apply(lambda row:self.remove_stopwords(row))\n",
    "        #test_tokens=test_tokens.apply(lambda row:self.remove_stopwords(row))\n",
    "        #train_tokens=train_tokens.apply(lambda row:self.remove_nonenglish(row))\n",
    "        #test_tokens=test_tokens.apply(lambda row:self.remove_nonenglish(row))  \n",
    "        train_tokens = train_tokens.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "        #test_tokens = test_tokens.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "        for i in range(len(train_tokens)):\n",
    "            train_tokens[i] = ' '.join(train_tokens[i])\n",
    "        '''for i in range(len(test_tokens)):\n",
    "            test_tokens[i] = ' '.join(test_tokens[i])'''\n",
    "\n",
    "        self.train['processed_sentiment1'] = train_tokens\n",
    "        #self.test['processed_sentiment1'] = test_tokens\n",
    "    def train_model_tfidf(self):\n",
    "        tfidf_vectorizer = TfidfVectorizer( max_features=1000, stop_words='english')\n",
    "        train_tfidf = tfidf_vectorizer.fit_transform(self.train['processed_sentiment1'])\n",
    "        #test_tfidf = tfidf_vectorizer.fit_transform(self.test['processed_sentiment1'])\n",
    "        xtrain, xval,ytrain,yval = train_test_split(train_tfidf,self.train['label'],test_size=0.50, random_state = 5)\n",
    "        self.lregt = LogisticRegression()\n",
    "        self.lregt.fit(xtrain,ytrain)\n",
    "        prediction = self.lregt.predict_proba(xval)\n",
    "        prediction_int = prediction[:,1] >= 0.3\n",
    "        prediction_int = prediction_int.astype(np.int)\n",
    "        print(\"fscore on validation set using features as tfidf\")\n",
    "        print(f1_score(yval, prediction_int))\n",
    "        \n",
    "        cnf_matrix = confusion_matrix(yval, prediction_int)\n",
    "        self.plot_confusion_matrix(cnf_matrix,[0,1],normalize=True,title=\"Confusion matrix\",model_type=\"tfidf\")\n",
    "        #predictions=self.lreg.predict(test_tfidf)\n",
    "        #self.test['predictions_tfidf']=predictions\n",
    "    def train_model_bow(self):\n",
    "        bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "        \n",
    "        train_bow = bow_vectorizer.fit_transform(self.train['processed_sentiment1'])\n",
    "        #test_bow = bow_vectorizer.fit_transform(self.test['processed_sentiment1'])\n",
    "        \n",
    "        xtrain, xval,ytrain,yval = train_test_split(train_bow,self.train['label'],test_size=0.50, random_state = 5)\n",
    "        \n",
    "        self.lreg = LogisticRegression()\n",
    "        self.lreg.fit(xtrain,ytrain)\n",
    "        prediction = self.lreg.predict_proba(xval)\n",
    "        prediction_int = prediction[:,1] >= 0.3\n",
    "        prediction_int = prediction_int.astype(np.int)\n",
    "        print(\"fscore on validation set using bag of words model as features\")\n",
    "        print(f1_score(yval, prediction_int))\n",
    "        \n",
    "        cnf_matrix = confusion_matrix(yval, prediction_int)\n",
    "        self.plot_confusion_matrix(cnf_matrix,[0,1],normalize=True,title=\"Confusion matrix\",model_type=\"bow\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        #predictions=self.lreg.predict(test_bow)\n",
    "        #self.test['predictions_bow']=predictions\n",
    "    def get_model_tfidf(self):\n",
    "        return self.lregt\n",
    "    def get_model_bow(self):\n",
    "        return self.lreg\n",
    "    #def get_results_bow(self):\n",
    "    #    return self.test\n",
    "    def plot_confusion_matrix(self,cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues,model_type=\"bow\"):\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            print(\"Normalized confusion matrix\")\n",
    "        else:\n",
    "            print('Confusion matrix, without normalization')\n",
    "\n",
    "        print(cm)\n",
    "\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "\n",
    "        fmt = '.2f' if normalize else 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.tight_layout()\n",
    "        fname=\"confusion_matrix\"+model_type\n",
    "        plt.savefig(fname)\n",
    "    def get_train_data(self):\n",
    "        return self.train\n",
    "    def get_test_data(self):\n",
    "        return self.test\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Abhay\n",
      "[nltk_data]     Nanda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Abhay\n",
      "[nltk_data]     Nanda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Study\\Ananconda\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#train=pd.read_csv('train.csv',encoding='latin-1')\n",
    "test=pd.read_csv('test.csv',encoding='latin-1')\n",
    "ts=twitter_sentiment()\n",
    "#ts.preprocess(train,2)\n",
    "filename=\"pickled_model_bow.sav\"\n",
    "#pickle.dump(ts.get_model_bow(),open(filename,\"wb\"))\n",
    "data=ts.preprocess_test(test,2)\n",
    "model=pickle.load(open(filename,\"rb\"))\n",
    "predictions=model.predict(data)\n",
    "test['predictionsbow']=predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Abhay\n",
      "[nltk_data]     Nanda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Abhay\n",
      "[nltk_data]     Nanda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Study\\Ananconda\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#train=pd.read_csv('train.csv',encoding='latin-1')\n",
    "ts=twitter_sentiment()\n",
    "#ts.preprocess(train,2)\n",
    "filename=\"pickled_model_tfidf.sav\"\n",
    "#pickle.dump(ts.get_model_bow(),open(filename,\"wb\"))\n",
    "data=ts.preprocess_test(test,2)\n",
    "model=pickle.load(open(filename,\"rb\"))\n",
    "predictions=model.predict(data)\n",
    "test['predictionstfidf']=predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
